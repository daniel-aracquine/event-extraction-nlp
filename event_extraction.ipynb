{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu_is53zj_au",
        "outputId": "dd81eaf7-5aef-4da8-a4be-9f79253a968b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.10/dist-packages (from newsapi-python) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0->newsapi-python) (3.4)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip3 install newsapi-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wVRl59Z0lIgT"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from newsapi import NewsApiClient\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cHgPEmW0lOQc"
      },
      "outputs": [],
      "source": [
        "api_key = '946472f8ad574082a31c9d0fcaf1e89d'\n",
        "newsapi = NewsApiClient(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SoddHCbYlSln"
      },
      "outputs": [],
      "source": [
        "def crawl_news(query):\n",
        "    all_results = []\n",
        "    for pag in tqdm(range(1, 6)):\n",
        "        pag_articles = newsapi.get_everything(q=query, sort_by='relevancy', page=pag)['articles']\n",
        "        if len(pag_articles) == 0:\n",
        "            break\n",
        "        all_results.extend(pag_articles)\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98ADci2alUra",
        "outputId": "16dfc11c-e3a2-4e8f-bed2-5e2219182b23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  3.37it/s]\n"
          ]
        }
      ],
      "source": [
        "tesla_news = crawl_news('tesla')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N9c_zbyRlXuR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('BBC news dataset.csv', usecols=range(1, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "moRaM-NMlaIs"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates('description', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "06AWCS4BlcgL"
      },
      "outputs": [],
      "source": [
        "descriptions = df['description'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QVT3BCQ_le0o"
      },
      "outputs": [],
      "source": [
        "def pre_process_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    # convert to lower case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove punctuation from each word\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    # filter out stop words\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFLezKNOlwa6",
        "outputId": "3eec2d87-b853-418f-b165-18e22fc2eb5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EeEw2H_lhhG",
        "outputId": "7b8ef5b8-7199-477b-881f-079ea791aee0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2128/2128 [00:04<00:00, 522.54it/s]\n"
          ]
        }
      ],
      "source": [
        "processed_descriptions = []\n",
        "for description in tqdm(descriptions):\n",
        "    processed_descriptions.append(' '.join(pre_process_text(description)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bdf6XJTl5jY",
        "outputId": "03e25247-e993-4281-c56a-f2f26b616c06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2128/2128 [01:39<00:00, 21.40it/s]\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "sent_vecs = {}\n",
        "docs = []\n",
        "\n",
        "for index, description in enumerate(tqdm(processed_descriptions)):\n",
        "    doc = nlp(description)\n",
        "    docs.append(doc)\n",
        "    sent_vecs[index] = doc.vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Bv5dUfZVl9eI"
      },
      "outputs": [],
      "source": [
        "vectors = list(sent_vecs.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PlGAW7U6l_IQ"
      },
      "outputs": [],
      "source": [
        "vectors = np.array(vectors)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
